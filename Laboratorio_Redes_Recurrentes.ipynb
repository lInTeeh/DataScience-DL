{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_6_Redes_Recurrentes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lph11h3K_jM"
      },
      "source": [
        "# RNNs\n",
        "\n",
        "En este laboratorio aprenderás a usar distintos tipos de redes recurrentes y la aplicarás para clasificar un conjunto de reviews de películas de IMDB en \"positivo\" o \"negativo\". Partimos importando los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLWy3bFpdDMV"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsIaljDeLFBD"
      },
      "source": [
        "Cargamos los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8LCquLEs53W"
      },
      "source": [
        "max_palabras = 30000\n",
        "a_train = 15000\n",
        "\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = tf.keras.datasets.imdb.load_data(num_words=max_palabras)\n",
        "\n",
        "# modificamos un poco los datasets para tener 40000 de train y 10000 de test\n",
        "X_train = np.concatenate((X_train, X_test[:a_train]), axis=0)\n",
        "Y_train = np.concatenate((Y_train, Y_test[:a_train]), axis=0)\n",
        "\n",
        "X_test = X_test[a_train:]\n",
        "Y_test = Y_test[a_train:]\n",
        "\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTIUF_laLJzU"
      },
      "source": [
        "Estas son funciones auxiliares para manejar textos y transformarlos en índices para poder trabajar más fácilmente con ellos. Si sabes un  poco de python y tienes tiempo,  dales una mirada. Son cosas standard que se deben hacer para procesar texto antes y después de pasárselos a una red neuronal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a54_Dg6iiUnq"
      },
      "source": [
        "palabra_a_id = keras.datasets.imdb.get_word_index()\n",
        "palabra_a_id = {k:(v+3) for k,v in palabra_a_id.items()}\n",
        "id_a_palabra = {v:k for k,v in palabra_a_id.items()}\n",
        "id_a_palabra[0] = '<PAD>'\n",
        "id_a_palabra[1] = '<INI>'\n",
        "id_a_palabra[2] = '<NN>'\n",
        "\n",
        "def ids_a_texto(lista):\n",
        "  lista_palabras = [id_a_palabra[id] for id in lista]\n",
        "  texto  = ' '.join(lista_palabras)\n",
        "  return texto\n",
        "\n",
        "def texto_a_ids(texto):\n",
        "  lista_palabras = texto.split()\n",
        "  lista = [1] + [\n",
        "      palabra_a_id[w] if w in palabra_a_id else 2\n",
        "      for w in lista_palabras\n",
        "  ]\n",
        "  return lista\n",
        "\n",
        "def texto_a_red_input(texto, max_largo=80):\n",
        "  lista = texto_a_ids(texto)\n",
        "  X = np.array(lista).reshape(1,-1)\n",
        "  X = keras.preprocessing.sequence.pad_sequences(X, maxlen=max_largo)\n",
        "  return X\n",
        "\n",
        "def posneg(i):\n",
        "  if i < 0.5:\n",
        "    return 'negativo'\n",
        "  else:\n",
        "    return 'positivo'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mGfMNzpLffZ"
      },
      "source": [
        "Hacemos un preprocesamiento de los datos para que todos los textos tengan el mismo largo (80 en este caso)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVNZ-JAYkyNf"
      },
      "source": [
        "max_largo = 80\n",
        "\n",
        "# mantenemos los originales\n",
        "X_train_original = X_train\n",
        "X_test_original = X_test\n",
        "\n",
        "# ahora los procesamos\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_largo)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_largo)\n",
        "print('X_train shape:', X_train.shape)\n",
        "print('X_test shape:', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1m1gAnJLVdH"
      },
      "source": [
        "Ejemplos de textos y sus etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSxbMnHotq5O"
      },
      "source": [
        "N = 5\n",
        "for _ in range(N):\n",
        "  i = np.random.randint(len(X_test))\n",
        "  print('original:', ids_a_texto(X_test_original[i]))\n",
        "  print('input red:', ids_a_texto(X_test[i]))\n",
        "  print('etiqueta:',posneg(Y_test[i]))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSt1ddq_L3VE"
      },
      "source": [
        "Importamos las capas recurrentes que usaremos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2WwYGkKvurP"
      },
      "source": [
        "from keras.layers import SimpleRNN, GRU, LSTM, Embedding, Dense, Dropout, Bidirectional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbNd06haODqK"
      },
      "source": [
        "Creamos nuestra primera red recurrente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Up9piZOv27A"
      },
      "source": [
        "np.random.seed(30)\n",
        "tf.random.set_seed(30)\n",
        "\n",
        "emb_dim = 32\n",
        "\n",
        "rnn = keras.Sequential()\n",
        "rnn.add(Embedding(max_palabras, emb_dim, input_length=max_largo))\n",
        "rnn.add(SimpleRNN(32))\n",
        "rnn.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#rnn.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbBe4z63OWHg"
      },
      "source": [
        "Compilamos y  entrenamos nuestra red (solo  por una época, si quieres puedes usar más pero las siguientes tardarán un tiempo considerablemente mayor). Usamos Adam como optimizador para mejorar las posibilidades de obtener una buena solución con pocas iteraciones  (solo en una época)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmuEcAsaw050"
      },
      "source": [
        "rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "rnn.fit(X_train, Y_train,\n",
        "          batch_size=32,\n",
        "          epochs=1,\n",
        "        validation_data=(X_test,Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scNnWxB0MT2W"
      },
      "source": [
        "Nuestro primer modelo llega a una certeza cercana al 83%.\n",
        "\n",
        "Mostramos las predicciones para algunos ejemplos. Note que ahora podemos pasarle como ejemplos textos propios para que pueda predecir. Si bien la certeza en el conjunto de prueba es bastante alta, las predicciones para ejemplos \"difíciles\" no son muy buenas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUH8DQWgMTd3"
      },
      "source": [
        "textos = [\n",
        "    \"this is a wonderful movie\",\n",
        "    \"this is a totally awful movie\",\n",
        "    \"this is an awful movie i really do not recommend it\",\n",
        "    \"this is the worst movie ever\",\n",
        "    \"i was expecting more way more\",\n",
        "    \"not what i expected but at the end i enjoyed it\",\n",
        "    \"i will see it again\",\n",
        "    \"i won't see it again\",\n",
        "    \"this is an excellent movie\",\n",
        "    \"if you are an idiot this is an excellent movie\",\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5e6coIrxGE3"
      },
      "source": [
        "for texto in textos:\n",
        "  X_in = texto_a_red_input(texto, max_largo)\n",
        "  pred = rnn.predict(X_in)\n",
        "  print(texto)\n",
        "  print(posneg(pred.item()), pred.item())\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7tpi-fGPsJe"
      },
      "source": [
        "# Ejercicio RNNs\n",
        "\n",
        "Trata de llegar a un 86% de certeza en el conjunto de prueba. Para esto prueba distintas opciones de arquitecturas (ojalá pruebes al menos 3 de creciente  complejidad, sin olvidar la regularización). Comienza aumentando las neuronas del embedding a 64 y luego prueba algunas de las siguientes opciones:\n",
        "\n",
        "- RNN con más neuronas (en embedding o en la capa recurrente)\n",
        "- GRU o LSTM\n",
        "- dos (o  mas?) capas de GRUs o LSTMs \n",
        "\n",
        "Si tienes más tiempo también puedes probar cambiando otros hiperparámetros (aunque sería bueno que para las  primeras redes te fijes más que nada en la arquitectura).\n",
        "\n",
        "y similar para `GRU` y `LSTM`. Para poder poner una capa recurrente sobre otra debes usar el parámetro `return_sequences=True`. Con esto le dices a la red que esperas que retorne la capa escondida en cada iteración y no solo al final. Por ejemplo el siguiente trozo de código usa tres capas recurrentes una sobre otra \n",
        "\n",
        "```\n",
        "...\n",
        "red.add(SimpleRNN(32, return_sequences=True))\n",
        "red.add(SimpleRNN(16, return_sequences=True))\n",
        "red.add(SimpleRNN(8))\n",
        "red.add(Dense(1))\n",
        "...\n",
        "```\n",
        "\n",
        "la primera de recurrencia simple y 32 neuronas, la segunda es una capa de 16 neuronas y la ultima una capa simple de 8 neuronas. Nota que la última capa recurrente no necesita el parámetro `return_sequences=True` pues solo  se usará la capa escondida final  para pasarla por  la última capa densa.\n",
        "\n",
        "Para cada red que crees, calcula el acierto en el conjunto de prueba y muestra algunas predicciones de frases escritas por ti para ver cómo se comporta en distintos casos. De los casos complicados de arriba, las predicciones deberían empezar a mejorar (al menos respecto de la probabilidad que  le asigna a ser positivo o negativo).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siX6xeP_6lg5"
      },
      "source": [
        "# Acá comienza todo tu código\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH5CmiYQ2znt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}